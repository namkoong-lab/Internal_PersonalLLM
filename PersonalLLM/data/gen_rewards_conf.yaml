debug: True
reward_model: "mistral_weqweasdas" #refer to constants.py for Alias
postreward_dataset_hfhub_name: null
gpus: [0,1,2,3,4,5,6,7]
start: 2
end: 50

dataset_names: ["namkoong-lab/PersonalLLM"]
prereward_dataset_hfhub_name: "andrewsiah/personalization_prompt_response"
cache_dir: "/shared/share_mala/andrew/huggingface/cache" #huggingface local cache
columns: 8 # number of columns of responses to score rewards with. ie 8 responses = 8 columns

batch_size: 1
# model_name: "Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback"
# tokenizer_name: "Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback"
tokenizer: null # this is automatically set, placed here only if manual override.
max_length: 8192 # if this is too short, rewards will be different due to truncation.
trust_remote_code: False