@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{castricato2024personareproducibletestbedpluralistic,
      title={PERSONA: A Reproducible Testbed for Pluralistic Alignment}, 
      author={Louis Castricato and Nathan Lile and Rafael Rafailov and Jan-Philipp Fränken and Chelsea Finn},
      year={2024},
      eprint={2407.17387},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.17387}, 
}

@misc{jang2023personalizedsoupspersonalizedlarge,
      title={Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging}, 
      author={Joel Jang and Seungone Kim and Bill Yuchen Lin and Yizhong Wang and Jack Hessel and Luke Zettlemoyer and Hannaneh Hajishirzi and Yejin Choi and Prithviraj Ammanabrolu},
      year={2023},
      eprint={2310.11564},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11564}, 
}

@misc{kirk2024prismalignmentprojectparticipatory,
      title={The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models}, 
      author={Hannah Rose Kirk and Alexander Whitefield and Paul Röttger and Andrew Bean and Katerina Margatina and Juan Ciro and Rafael Mosquera and Max Bartolo and Adina Williams and He He and Bertie Vidgen and Scott A. Hale},
      year={2024},
      eprint={2404.16019},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16019}, 
}

@misc{chan2024scalingsyntheticdatacreation,
      title={Scaling Synthetic Data Creation with 1,000,000,000 Personas}, 
      author={Xin Chan and Xiaoyang Wang and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2406.20094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.20094}, 
}

}

@misc{ie2019recsimconfigurablesimulationplatform,
      title={RecSim: A Configurable Simulation Platform for Recommender Systems}, 
      author={Eugene Ie and Chih-wei Hsu and Martin Mladenov and Vihan Jain and Sanmit Narvekar and Jing Wang and Rui Wu and Craig Boutilier},
      year={2019},
      eprint={1909.04847},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.04847}, 
}

@misc{min2022rethinkingroledemonstrationsmakes,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.12837}, 
}

@misc{yoo2022groundtruthlabelsmatterdeeper,
      title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations}, 
      author={Kang Min Yoo and Junyeob Kim and Hyuhng Joon Kim and Hyunsoo Cho and Hwiyeol Jo and Sang-Woo Lee and Sang-goo Lee and Taeuk Kim},
      year={2022},
      eprint={2205.12685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.12685}, 
}

@misc{pan2023incontextlearninglearnsincontext,
      title={What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning}, 
      author={Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
      year={2023},
      eprint={2305.09731},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09731}, 
}

@inproceedings{wu-etal-2021-personalized,
    title = "Personalized Response Generation via Generative Split Memory Network",
    author = "Wu, Yuwei  and
      Ma, Xuezhe  and
      Yang, Diyi",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.157",
    doi = "10.18653/v1/2021.naacl-main.157",
    pages = "1956--1970",
    abstract = "Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain \textit{single-turn} dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.",
}


@inproceedings{mazare-etal-2018-training,
    title = "Training Millions of Personalized Dialogue Agents",
    author = "Mazar{\'e}, Pierre-Emmanuel  and
      Humeau, Samuel  and
      Raison, Martin  and
      Bordes, Antoine",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1298",
    doi = "10.18653/v1/D18-1298",
    pages = "2775--2779",
    abstract = "Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and only contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",
}

@misc{muennighoff2023mtebmassivetextembedding,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}


@misc{gebru2021datasheets,
      title={Datasheets for Datasets}, 
      author={Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III au2 and Kate Crawford},
      year={2021},
      eprint={1803.09010},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}

@inproceedings{ao-etal-2021-pens,
    title = "{PENS}: A Dataset and Generic Framework for Personalized News Headline Generation",
    author = "Ao, Xiang  and
      Wang, Xiting  and
      Luo, Ling  and
      Qiao, Ying  and
      He, Qing  and
      Xie, Xing",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.7",
    doi = "10.18653/v1/2021.acl-long.7",
    pages = "82--92",
    abstract = "In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user{'}s reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. We propose a generic framework as a preparatory solution to our problem. At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. The dataset is available at \url{https://msnews.github.io/pens.html}.",
}


@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@misc{zhao2023kuaisimcomprehensivesimulatorrecommender,
      title={KuaiSim: A Comprehensive Simulator for Recommender Systems}, 
      author={Kesen Zhao and Shuchang Liu and Qingpeng Cai and Xiangyu Zhao and Ziru Liu and Dong Zheng and Peng Jiang and Kun Gai},
      year={2023},
      eprint={2309.12645},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2309.12645}, 
}



@misc{wang2023helpsteermultiattributehelpfulnessdataset,
      title={HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM}, 
      author={Zhilin Wang and Yi Dong and Jiaqi Zeng and Virginia Adams and Makesh Narsimhan Sreedhar and Daniel Egert and Olivier Delalleau and Jane Polak Scowcroft and Neel Kant and Aidan Swope and Oleksii Kuchaiev},
      year={2023},
      eprint={2311.09528},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09528}, 
}

@misc{li2024personalizedlanguagemodelingpersonalized,
      title={Personalized Language Modeling from Personalized Human Feedback}, 
      author={Xinyu Li and Zachary C. Lipton and Liu Leqi},
      year={2024},
      eprint={2402.05133},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.05133}, 
}

@misc{lin2023unlockingspellbasellms,
      title={The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning}, 
      author={Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
      year={2023},
      eprint={2312.01552},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.01552}, 
}

@inproceedings{bird-loper-2004-nltk,
    title = "{NLTK}: The Natural Language Toolkit",
    author = "Bird, Steven  and
      Loper, Edward",
    booktitle = "Proceedings of the {ACL} Interactive Poster and Demonstration Sessions",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P04-3031",
    pages = "214--217",
}


@misc{russakovsky2015imagenetlargescalevisual,
      title={ImageNet Large Scale Visual Recognition Challenge}, 
      author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
      year={2015},
      eprint={1409.0575},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.0575}, 
}

@misc{sun2024personadbefficientlargelanguage,
      title={Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement}, 
      author={Chenkai Sun and Ke Yang and Revanth Gangi Reddy and Yi R. Fung and Hou Pong Chan and ChengXiang Zhai and Heng Ji},
      year={2024},
      eprint={2402.11060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11060}, 
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Google},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{perez2022red,
      title={Red Teaming Language Models with Language Models}, 
      author={Ethan Perez and Saffron Huang and Francis Song and Trevor Cai and Roman Ring and John Aslanides and Amelia Glaese and Nat McAleese and Geoffrey Irving},
      year={2022},
      eprint={2202.03286},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tan2024democratizing,
      title={Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning}, 
      author={Zhaoxuan Tan and Qingkai Zeng and Yijun Tian and Zheyuan Liu and Bing Yin and Meng Jiang},
      year={2024},
      eprint={2402.04401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ziegler2020finetuning,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{christiano2023deep,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{wei2022finetuned,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2022},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{2212.09251,
Author = {Ethan Perez and Sam Ringer and Kamilė Lukošiūtė and Karina Nguyen and Edwin Chen and Scott Heiner and Craig Pettit and Catherine Olsson and Sandipan Kundu and Saurav Kadavath and Andy Jones and Anna Chen and Ben Mann and Brian Israel and Bryan Seethor and Cameron McKinnon and Christopher Olah and Da Yan and Daniela Amodei and Dario Amodei and Dawn Drain and Dustin Li and Eli Tran-Johnson and Guro Khundadze and Jackson Kernion and James Landis and Jamie Kerr and Jared Mueller and Jeeyoon Hyun and Joshua Landau and Kamal Ndousse and Landon Goldberg and Liane Lovitt and Martin Lucas and Michael Sellitto and Miranda Zhang and Neerav Kingsland and Nelson Elhage and Nicholas Joseph and Noemí Mercado and Nova DasSarma and Oliver Rausch and Robin Larson and Sam McCandlish and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Brown and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Jack Clark and Samuel R. Bowman and Amanda Askell and Roger Grosse and Danny Hernandez and Deep Ganguli and Evan Hubinger and Nicholas Schiefer and Jared Kaplan},
Title = {Discovering Language Model Behaviors with Model-Written Evaluations},
Year = {2022},
Eprint = {arXiv:2212.09251},
}

@misc{2209.07065,
Author = {Hang Jiang and Doug Beeferman and Brandon Roy and Deb Roy},
Title = {CommunityLM: Probing Partisan Worldviews from Language Models},
Year = {2022},
Eprint = {arXiv:2209.07065},
}

@misc{2307.14324,
Author = {Nino Scherrer and Claudia Shi and Amir Feder and David M. Blei},
Title = {Evaluating the Moral Beliefs Encoded in LLMs},
Year = {2023},
Eprint = {arXiv:2307.14324},
}

@misc{2301.01768,
Author = {Jochen Hartmann and Jasper Schwenzow and Maximilian Witte},
Title = {The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation},
Year = {2023},
Eprint = {arXiv:2301.01768},
}

@misc{Hu2021,
Author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
Title = {LoRA: Low-Rank Adaptation of Large Language Models},
Year = {2021},
Eprint = {arXiv:2106.09685},
}

@misc{rafailov2023direct,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Valipour2022,
Author = {Mojtaba Valipour and Mehdi Rezagholizadeh and Ivan Kobyzev and Ali Ghodsi},
Title = {DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation},
Year = {2022},
Eprint = {arXiv:2210.07558},
}

@misc{Liu2024,
Author = {Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
Title = {DoRA: Weight-Decomposed Low-Rank Adaptation},
Year = {2024},
Eprint = {arXiv:2402.09353},
}

@misc{dubois2024alpacafarm,
      title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
      author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
      year={2024},
      eprint={2305.14387},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{köpf2023openassistant,
      title={OpenAssistant Conversations -- Democratizing Large Language Model Alignment}, 
      author={Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
      year={2023},
      eprint={2304.07327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{nakano2022webgpt,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{davidson2010youtube,
  title={The YouTube video recommendation system},
  author={Davidson, James and Liebald, Benjamin and Liu, Junning and Nandy, Palash and Van Vleet, Taylor and Gargi, Ullas and Gupta, Sujoy and He, Yu and Lambert, Mike and Livingston, Blake and others},
  booktitle={Proceedings of the fourth ACM conference on Recommender systems},
  pages={293--296},
  year={2010}
}

@inproceedings{das2007google,
  title={Google news personalization: scalable online collaborative filtering},
  author={Das, Abhinandan S and Datar, Mayur and Garg, Ashutosh and Rajaram, Shyam},
  booktitle={Proceedings of the 16th international conference on World Wide Web},
  pages={271--280},
  year={2007}
}

@inproceedings{Xu_2022, series={RecSys ’22},
   title={Rethinking Personalized Ranking at Pinterest: An End-to-End Approach},
   url={http://dx.doi.org/10.1145/3523227.3547394},
   DOI={10.1145/3523227.3547394},
   booktitle={Proceedings of the 16th ACM Conference on Recommender Systems},
   publisher={ACM},
   author={Xu, Jiajing and Zhai, Andrew and Rosenberg, Charles},
   year={2022},
   month=sep, collection={RecSys ’22} }

@article{F_rber_2020,
   title={Citation recommendation: approaches and datasets},
   volume={21},
   ISSN={1432-1300},
   url={http://dx.doi.org/10.1007/s00799-020-00288-2},
   DOI={10.1007/s00799-020-00288-2},
   number={4},
   journal={International Journal on Digital Libraries},
   publisher={Springer Science and Business Media LLC},
   author={Färber, Michael and Jatowt, Adam},
   year={2020},
   month=aug, pages={375–405} }

@misc{salemi2024optimization,
      title={Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation}, 
      author={Alireza Salemi and Surya Kallumadi and Hamed Zamani},
      year={2024},
      eprint={2404.05970},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{salemi2024lamp,
      title={LaMP: When Large Language Models Meet Personalization}, 
      author={Alireza Salemi and Sheshera Mysore and Michael Bendersky and Hamed Zamani},
      year={2024},
      eprint={2304.11406},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mazaré2018training,
      title={Training Millions of Personalized Dialogue Agents}, 
      author={Pierre-Emmanuel Mazaré and Samuel Humeau and Martin Raison and Antoine Bordes},
      year={2018},
      eprint={1809.01984},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2018personalizing,
      title={Personalizing Dialogue Agents: I have a dog, do you have pets too?}, 
      author={Saizheng Zhang and Emily Dinan and Jack Urbanek and Arthur Szlam and Douwe Kiela and Jason Weston},
      year={2018},
      eprint={1801.07243},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{lambert2024rewardbench,
      title={RewardBench: Evaluating Reward Models for Language Modeling}, 
      author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2403.13787},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hwang2023aligninglanguagemodelsuser,
      title={Aligning Language Models to User Opinions}, 
      author={EunJeong Hwang and Bodhisattwa Prasad Majumder and Niket Tandon},
      year={2023},
      eprint={2305.14929},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14929}, 
}


@misc{eisenstein2023helpingherdingrewardmodel,
      title={Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking}, 
      author={Jacob Eisenstein and Chirag Nagpal and Alekh Agarwal and Ahmad Beirami and Alex D'Amour and DJ Dvijotham and Adam Fisch and Katherine Heller and Stephen Pfohl and Deepak Ramachandran and Peter Shaw and Jonathan Berant},
      year={2023},
      eprint={2312.09244},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.09244}, 
}

@misc{tan2024personalizedpiecesefficientpersonalized,
      title={Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts}, 
      author={Zhaoxuan Tan and Zheyuan Liu and Meng Jiang},
      year={2024},
      eprint={2406.10471},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10471}, 
}

@misc{coste2024rewardmodelensembleshelp,
      title={Reward Model Ensembles Help Mitigate Overoptimization}, 
      author={Thomas Coste and Usman Anwar and Robert Kirk and David Krueger},
      year={2024},
      eprint={2310.02743},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.02743}, 
}


@misc{santurkar2023opinions,
      title={Whose Opinions Do Language Models Reflect?}, 
      author={Shibani Santurkar and Esin Durmus and Faisal Ladhak and Cinoo Lee and Percy Liang and Tatsunori Hashimoto},
      year={2023},
      eprint={2303.17548},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wang2024limaml,
      title={LiMAML: Personalization of Deep Recommender Models via Meta Learning}, 
      author={Ruofan Wang and Prakruthi Prabhakar and Gaurav Srivastava and Tianqi Wang and Zeinab S. Jalali and Varun Bharill and Yunbo Ouyang and Aastha Nigam and Divya Venugopalan and Aman Gupta and Fedor Borisyuk and Sathiya Keerthi and Ajith Muralidharan},
      year={2024},
      eprint={2403.00803},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{chen2023large,
      title={When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities}, 
      author={Jin Chen and Zheng Liu and Xu Huang and Chenwang Wu and Qi Liu and Gangwei Jiang and Yuanhao Pu and Yuxuan Lei and Xiaolong Chen and Xingmei Wang and Defu Lian and Enhong Chen},
      year={2023},
      eprint={2307.16376},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}



@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}

@misc{ji2023beavertails,
      title={BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset}, 
      author={Jiaming Ji and Mickel Liu and Juntao Dai and Xuehai Pan and Chi Zhang and Ce Bian and Chi Zhang and Ruiyang Sun and Yizhou Wang and Yaodong Yang},
      year={2023},
      eprint={2307.04657},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings {extract,
author = {Nicholas Carlini and Florian Tram{\`e}r and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and {\'U}lfar Erlingsson and Alina Oprea and Colin Raffel},
title = {Extracting Training Data from Large Language Models},
booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
year = {2021},
isbn = {978-1-939133-24-3},
pages = {2633--2650},
url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
publisher = {USENIX Association},
month = aug
}
