
    

\section{Dataset Information}\label{app:add_data_det}

This section serves as documentation for our dataset, with its organization based on the notion of datasheets \citep{gebru2021datasheets}.

Our dataset is available at \url{https://huggingface.co/datasets/namkoong-lab/PersonalLLM}.

The code used to collect, process, and score our dataset (and run our experiments) is available at \url{https://github.com/namkoong-lab/PersonalLLM}.

The evaluation dataset that we used to produce our experiments in Section 4 is available at \url{https://huggingface.co/datasets/namkoong-lab/PersonalLLM_Eval}.

The interaction database dataset that we constructed from PersonalLLM is available at \url{https://huggingface.co/datasets/namkoong-lab/PersonalLLM_InteractionDatabase}.

\subsection{Composition}

\subsubsection{Prompts}

In order to create a set of prompts and responses over which humans (and reward models) will display diverse preferences, our first focus was the collection of \textit{open-ended} prompts.
As a source of these open-ended prompts, we collected data from \textbf{Anthropic Helpful-online}, \textbf{Anthropic Helpful-base}, \textbf{Nvidia Helpsteer}, and \textbf{RewardBench}.
From this set, prompts were filtered to those with a length of 2400 characters or fewer as most reward models are limited to 4096 context length. We randomly drew 10,402 prompts from the filtered subset.  The resulting distribution of prompts from different sources is shown in Table~\ref{tab:prompt_sources}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{cc}
        \toprule
        Source & Count \\
        \midrule
        hh-rlhf-helpful-base & 4797 \\
        hh-rlhf-helpful-online & 3320 \\
        HelpSteer & 1290 \\
        alpacaeval-hard & 285 \\
        alpacaeval-easy & 259 \\
        alpacaeval-length & 219 \\
        xstest-should-respond & 65 \\
        llmbar-adver-neighbor & 43 \\
        llmbar-adver-GPTInst & 34 \\
        llmbar-natural & 27 \\
        llmbar-adver-manual & 19 \\
        llmbar-adver-GPTOut & 15 \\
        mt-bench-med & 14 \\
        mt-bench-hard & 10 \\
        mt-bench-easy & 5 \\
        \bottomrule
    \end{tabular}
    \caption{Sources of the 10,402 prompts composing our train and test sets.}
    \label{tab:prompt_sources}
\end{table}

\subsubsection{Responses}

Next, we aimed to collect many high-quality responses for each prompt. 
We generated eight responses for each of the 10,402 prompts using a selection of the top models from ChatArena and other important benchmarks: \textbf{GPT-4o}, \textbf{Claude 3 Opus}, \textbf{Gemini-Pro-1.5}, \textbf{Command-R-Plus}, \textbf{GPT-4-Turbo}, \textbf{Claude 3 Sonnet}, \textbf{Llama3-70B-Instruct}, and \textbf{Mixtral 8x22B}. We split the resulting dataset into training and test sets in a roughly 9:1 ratio, with a final count of 9,402 training examples and 1,000 test examples.

\subsubsection{Reward Models}

Finally, in order to enable the simulation of many diverse preference models, we select 10 reward models from Reward Bench, built on top of popular base models such as Llama3, Mistral, and Gemma.  Their model names on Huggingface are:  

\begin{itemize}
    \item hendrydong/Mistral-RM-for-RAFT-GSHF-v0 
    \item OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1
    \item OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5
    \item OpenAssistant/reward-model-deberta-v3-large-v2
    \item PKU-Alignment/beaver-7b-v1.0-cost 
    \item Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback
    \item sfairXC/FsfairX-LLaMA3-RM-v0.1
    \item weqweasdas/RM-Gemma-2B 
    \item weqweasdas/RM-Gemma-7B 
    \item weqweasdas/RM-Mistral-7B 
\end{itemize}

We evaluate each (prompt, response) pair in the train and test set with each model so that for any personality created by their ensembling, each (prompt, response) pair in the dataset can be scored via a simple weighting.

\subsubsection{Data Records}

Each record in our resulting dataset is of the form
$$ (x,s,y_1,r_1^{(1)},...,r_1^{(k)},...,y_l,r_l^{(1)},...,r_l^{(k)}) $$
where $x$ is an input prompt, $s$ gives the source dataset for the prompt, $y_i$ is a response generated by the LLM indexed by $i$ among our set of $k=8$, and $r_i^{(j)}$ is the reward score for prompt/response pair $(x, y_i)$ by the reward model indexed by $j$ among a set of $l=10$ reward models in total.


\subsection{Collection Process}

Our prompts were collected by downloading and filtering the open-source datasets mentioned previously.  Responses were generated using OpenRouter with a temperature of 1.0 and a maximum token length of 512.

\subsection{Preprocessing/cleaning/labeling}

All prompts are taken in their exact form from existing open-source datasets, filtered by length according to Appendix A.1.1.  LLM responses are not filtered, edited, or cleaned in any way, either for storage or reward scoring.  

As a limitation, we note that all prompts and responses have not been manually inspected for quality or safety by a human, although prompts are sourced from existing, reputable datasets, and responses are generated from state-of-the-art language models that have (presumably in the case of black box models) undergone safety alignment.  Further, the personas that can be created via ensembling our reward models have not been tested for bias or alignment with a particular subgroup of the population.

We also note that there is a known issue with many reward models, such that they may produce different scores under different conditions, in particular when the batch size changes \footnote{https://github.com/allenai/reward-bench/issues/137}.  Our reward scores are produced with a batch size of 1 and are tested for reproducibility and determinism.

\subsection{Uses}

Our goal in creating the open-source PersonalLLM dataset is to facilitate work on methods to personalize the output of an LLM to the individual tastes of the many diverse users of an application.
In our initial paper, we have provided experiments where meta-learning and in-context learning (ICL) are used to leverage an existing user base with interaction histories to improve outcomes for new users.
We imagine further work in this direction, as well as potential work on more efficient ways to harness the power of fine-tuning for personalization. Also, in domains like medicine, where privacy is paramount, it may not be possible to include queries from other users in context.  Thus, work on privacy-ensuring meta-learning personalization algorithms is needed. 

It must be acknowledged that the goal of LLM personalization brings particular risks, including filter bubbles, stereotyping, feedback loops, personification, and manipulation.
Given these and many other predictable (and unpredictable) potential risks, it is important that any efforts at LLM personalization are accompanied by research in robust transparency mechanisms and safeguards for personalization algorithms.

\subsection{Distribution}

\subsubsection{Hosting}

PersonalLLM is available for download on huggingface at \url{https://huggingface.co/datasets/namkoong-lab/PersonalLLM}.

\subsubsection{License}

We release this dataset under a CC BY-NC 4.0 License, which prohibits commercial use and requires attribution.

\subsection{Maintenance}

The authors plan to maintain the dataset.  If any instances of dangerous, private, or otherwise undesirable material are found, the corresponding row in the dataset will be deleted.

Correspondence, including requests for data removal, can be sent to \url{andrew.siah@columbia.edu} and \url{tpz2105@columbia.edu}.


\section{Simulated User Analysis}\label{app:users}

\subsection{Additional Details}\label{app:user_details}

All semantic features are scored using pre-trained models from Huggingface.

\begin{itemize}
    \item Formality is scored using: s-nlp/roberta-base-formality-ranker
    \item Educational value is scored using: HuggingFaceFW/fineweb-edu-classifier
    \item Emotions are scored using: SamLowe/roberta-base-go\_emotions
\end{itemize}

Below is a list of our syntactic features:
\begin{itemize}
    \item Count of tokens
    \item Count of unique words
    \item Average word length
    \item Count of stopwords
    \item Count of punctuation
    \item Count of list items (bullets or numbered)
    \item Count of adjectives and Adverbs
\end{itemize}
The python package nltk \citep{bird-loper-2004-nltk} is used to tokenize, extract stopwords, and tag parts of speech, where necessary.

Our linear regression models are built using sklearn \citep{scikit-learn}, with default parameter settings.

\subsection{Additional Results}\label{app:user_results}

Tables~\ref{tab:opinion_qa_full} and \ref{tab:opinion_qa_full_2} show representativeness scores for our PersonalLLM users as well as a selection of LLMs across all 60 demographic groups in the original OpinionQA \citep{santurkar2023opinions} study.

\begin{table}[!ht]
    \centering
    \begin{tabular}{lccccc}
    \toprule
     & \multicolumn{2}{c}{AI21 Labs} & \multicolumn{2}{c}{OpenAI} & \textsf{PersonalLLM} \\
    Demographic & j1-jumbo & j1-grande-v2 & ada & text-davinci-003 & \textbf{Ours} \\
    \midrule
Northeast & 0.811 & 0.802 & 0.819 & 0.704 & 0.838 \\
Midwest & 0.810 & 0.797 & 0.820 & 0.701 & 0.833 \\
South & 0.818 & 0.805 & 0.827 & 0.696 & 0.835 \\
West & 0.813 & 0.802 & 0.821 & 0.704 & 0.839 \\
18-29 & 0.818 & 0.808 & 0.828 & 0.700 & 0.840 \\
30-49 & 0.814 & 0.804 & 0.823 & 0.702 & 0.837 \\
50-64 & 0.809 & 0.797 & 0.818 & 0.696 & 0.830 \\
65+ & 0.792 & 0.779 & 0.800 & 0.699 & 0.818 \\
Male & 0.814 & 0.802 & 0.826 & 0.697 & 0.837 \\
Female & 0.810 & 0.800 & 0.816 & 0.702 & 0.833 \\
Less than high school & 0.828 & 0.812 & 0.835 & 0.685 & 0.832 \\
High school graduate & 0.816 & 0.799 & 0.826 & 0.691 & 0.832 \\
Some college, no degree & 0.814 & 0.804 & 0.823 & 0.701 & 0.836 \\
Associate's degree & 0.811 & 0.800 & 0.821 & 0.700 & 0.834 \\
College graduate & 0.802 & 0.794 & 0.810 & 0.710 & 0.833 \\
Postgraduate & 0.794 & 0.789 & 0.800 & 0.717 & 0.831 \\
Citizen - Yes & 0.814 & 0.802 & 0.823 & 0.700 & 0.836 \\
Citizen - No & 0.816 & 0.812 & 0.818 & 0.706 & 0.833 \\
Married & 0.810 & 0.799 & 0.819 & 0.699 & 0.832 \\
Divorced & 0.809 & 0.796 & 0.817 & 0.696 & 0.830 \\
Separated & 0.814 & 0.801 & 0.818 & 0.694 & 0.830 \\
Widowed & 0.800 & 0.785 & 0.807 & 0.694 & 0.819 \\
Never been married & 0.819 & 0.808 & 0.828 & 0.700 & 0.841 \\
Protestant & 0.810 & 0.797 & 0.820 & 0.694 & 0.828 \\
Roman Catholic & 0.816 & 0.806 & 0.823 & 0.702 & 0.835 \\
Mormon & 0.789 & 0.777 & 0.802 & 0.696 & 0.819 \\
Orthodox & 0.773 & 0.762 & 0.781 & 0.693 & 0.803 \\
Jewish & 0.792 & 0.785 & 0.800 & 0.707 & 0.824 \\
Muslim & 0.794 & 0.788 & 0.792 & 0.697 & 0.816 \\
Buddhist & 0.782 & 0.777 & 0.783 & 0.709 & 0.821 \\
Hindu & 0.796 & 0.794 & 0.789 & 0.707 & 0.816 \\
Atheist & 0.774 & 0.771 & 0.784 & 0.714 & 0.822 \\
Agnostic & 0.785 & 0.781 & 0.794 & 0.717 & 0.828 \\
Other & 0.794 & 0.790 & 0.801 & 0.703 & 0.824 \\
Nothing in particular & 0.815 & 0.802 & 0.824 & 0.700 & 0.839 \\
Rel. attend - $>$1x/week & 0.807 & 0.793 & 0.816 & 0.690 & 0.824 \\
Rel. attend - 1x/week & 0.811 & 0.798 & 0.819 & 0.696 & 0.829 \\
Rel. attend - 1-2x/month & 0.818 & 0.807 & 0.825 & 0.699 & 0.833 \\
Rel. attend - Few/year & 0.817 & 0.809 & 0.824 & 0.705 & 0.837 \\
Rel. attend - Seldom & 0.811 & 0.800 & 0.821 & 0.703 & 0.835 \\
Rel. attend - Never & 0.806 & 0.795 & 0.816 & 0.701 & 0.836 \\
    \bottomrule
    \end{tabular}
    \caption{Representativeness scores in relation to real human opinions from important demographic groups for different LLMs, as well as our \textsf{PersonalLLM} population.}
    \label{tab:opinion_qa_full}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{lccccc}
    \toprule
     & \multicolumn{2}{c}{AI21 Labs} & \multicolumn{2}{c}{OpenAI} & \textsf{PersonalLLM} \\
    Demographic & j1-jumbo & j1-grande-v2 & ada & text-davinci-003 & \textbf{Ours} \\
    \midrule
    Republican & 0.791 & 0.776 & 0.805 & 0.680 & 0.812 \\
    Democrat & 0.800 & 0.795 & 0.804 & 0.719 & 0.834 \\
    Independent & 0.812 & 0.801 & 0.821 & 0.701 & 0.838 \\
    Other & 0.820 & 0.804 & 0.832 & 0.693 & 0.839 \\
    Less than \$30,000 & 0.828 & 0.813 & 0.833 & 0.693 & 0.838 \\
    \$30,000-\$50,000 & 0.814 & 0.802 & 0.822 & 0.698 & 0.834 \\
    \$50,000-\$75,000 & 0.807 & 0.796 & 0.816 & 0.703 & 0.833 \\
    \$75,000-\$100,000 & 0.800 & 0.791 & 0.811 & 0.705 & 0.829 \\
    \$100,000 or more & 0.797 & 0.790 & 0.807 & 0.708 & 0.831 \\
Very conservative & 0.797 & 0.778 & 0.811 & 0.662 & 0.811 \\
Conservative & 0.796 & 0.780 & 0.810 & 0.684 & 0.817 \\
Moderate & 0.814 & 0.804 & 0.822 & 0.706 & 0.838 \\
Liberal & 0.792 & 0.788 & 0.799 & 0.721 & 0.833 \\
Very liberal & 0.785 & 0.782 & 0.791 & 0.712 & 0.825 \\
White & 0.807 & 0.794 & 0.817 & 0.699 & 0.832 \\
Black & 0.820 & 0.812 & 0.823 & 0.702 & 0.833 \\
Asian & 0.814 & 0.806 & 0.819 & 0.708 & 0.839 \\
Hispanic & 0.820 & 0.810 & 0.824 & 0.706 & 0.839 \\
Other & 0.801 & 0.783 & 0.807 & 0.681 & 0.818 \\
    \bottomrule
    \end{tabular}
    \caption{Representativeness scores in relation to real human opinions from important demographic groups for different LLMs, as well as our \textsf{PersonalLLM} population.}
    \label{tab:opinion_qa_full_2}
\end{table}

\section{Experiments}\label{app:exp}


\subsection{Pseudocode}\label{app:exp_details}

Below is the pseudocode for the baselines in Section 4. Actual code is available at \url{https://github.com/namkoong-lab/PersonalLLM}.

\begin{algorithm}
\caption{MetaLearnKShotICLAlgorithm}
\begin{algorithmic}[1]
\Procedure{GenerateResponse}{\text{text\_user}, \text{text\_prompt}}
    \State \text{similar\_users} $\gets$ \text{FindSimilarUsers}(\text{text\_user})
    \State \text{similar\_prompts} $\gets$ \text{FindSimilarPrompts}(\text{text\_prompt, similar\_users})
    \State \text{icl\_examples} $\gets \{\}$
    \For{\text{prompt} \textbf{in} \text{similar\_prompts}}
        \State \text{winning, losing} $\gets$ \text{FindWinningLosingResponses}(\text{prompt})
        \State \text{icl\_examples}.append(\text{prompt, winning, losing})
        \If{$|\text{icl\_examples}| = k$} \Comment{\text{k is the number of shots}}
            \State \textbf{break}
        \EndIf
    \EndFor
    \State \text{final\_prompt} $\gets$ \text{ConstructPrompt}(\text{icl\_examples, test\_prompt})
    \State \text{response} $\gets$ \text{GenerateLLMResponse}(\text{final\_prompt})
    \State \Return \text{response}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\newpage
\subsection{Prompt Template}

Below is a prompt template we used in our experiments for winning and losing responses appended during inference.
\vspace{1cm}
\hrule
\begin{verbatim}
Below are some examples of past conversations with liked and disliked responses
per prompt. 

User: {ICL_Prompt_1}
Liked Response: {Prompt_1_Liked_Response}
Disliked Response: {Prompt_1_Disliked_Response}

User: {ICL_Prompt_2}
Liked Response: {Prompt_2_Liked_Response}
Disliked Response: {Prompt_2_Disliked_Response}

Use the contexts above to generate a good response for the user prompt below. 
Your response should be similar to the winning responses and dissimilar from
the losing responses. 

User: {Test_prompt}
Response: 
\end{verbatim}
\hrule
\vspace{1cm}
Below is a prompt template we used in our experiments for winning responses only appended during inference.
\vspace{1cm}
\hrule
\begin{verbatim}
Below are some examples of past conversations with liked responses per prompt. 

User: {ICL_Prompt_1}
Liked Response: {Prompt_1_Liked_Response}

User: {ICL_Prompt_2}
Liked Response: {Prompt_2_Liked_Response}

Use the contexts above to generate a good response for the user prompt below. 
Your response should be similar to the winning responses.

User: {Test_prompt}
Response: 
\end{verbatim}
\hrule